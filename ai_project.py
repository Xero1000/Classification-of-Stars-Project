# -*- coding: utf-8 -*-
"""AI Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LlL9iYEr1on1Ve5DjqXw9k4IkQQJmU4o

## Spectral Type Character to Integer Conversion Function
"""

# Spectral Class characters are from the HR Diagram
# Order in the Morgan-Keenan diagram is O, B, A, F, G, K, M
# This function swaps out the letters with integers
def spTypeNum(x):
  i = 0
  while i < len(x):
    if x.at[i, 'Spectral_Class'] == 'O':
      x.at[i, 'Spectral_Class'] = 1
    elif x.at[i, 'Spectral_Class'] == 'B':
      x.at[i, 'Spectral_Class'] = 2
    elif x.at[i, 'Spectral_Class'] == 'A':
      x.at[i, 'Spectral_Class'] = 3
    elif x.at[i, 'Spectral_Class'] == 'F':
      x.at[i, 'Spectral_Class'] = 4
    elif x.at[i, 'Spectral_Class'] == 'G':
      x.at[i, 'Spectral_Class'] = 5
    elif x.at[i, 'Spectral_Class'] == 'K':
      x.at[i, 'Spectral_Class'] = 6
    elif x.at[i, 'Spectral_Class'] == 'M':
      x.at[i, 'Spectral_Class'] = 7
    i = i + 1
  return x

"""## Create Confusion Matrix Function"""

def createConfusionMatrix(y_test, predicted_classes):
  # Predicted classes on top
  # Actual classes on side
  # Index = star type (index 0 = Brown Dwarf, 1 = Red Dwarf, 2 = White Dwarf,
  # 3 = Main Sequence Star, 4 = Supergiant, and 5 = Hypergiant)
  confusion_matrix = [[0, 0, 0, 0, 0, 0], 
                      [0, 0, 0, 0, 0, 0], 
                      [0, 0, 0, 0, 0, 0], 
                      [0, 0, 0, 0, 0, 0], 
                      [0, 0, 0, 0, 0, 0], 
                      [0, 0, 0, 0, 0, 0]]
  #row = actual class
  #column = predicted class
  for i in range(len(y_test)):
    confusion_matrix[y_test[i]][predicted_classes[i]] += 1
  
  #print the confusion matrix
  for x in confusion_matrix:
    print(x)

"""## Naive Bayes

"""

def naiveBayes(x_train, x_test, y_train, y_test):
    
    # index positions from before the split are maintained
    # reset_index resets all the index values to restore proper ordering
    x_train = x_train.reset_index(drop=True)
    x_test = x_test.reset_index(drop=True)
    y_train = y_train.reset_index(drop=True)
    y_test = y_test.reset_index(drop=True)

    quantities = [0, 0, 0, 0, 0, 0] # The number of occurences of each class (index of value represents the class)
    means = [] # row = column, column = mean for each class within the column for columns 0-3

    stds = [] # standard deviation for columns 0-3

    # Lists storing the unique values of Color and Spectral_class columns
    colorKeys = x_train.Color.unique()
    spTypeKeys = x_train.Spectral_Class.unique()

    # Dictionaries storing the number of occurences of each unique Color and spectral_class value
    colorDicList = []
    spTypeDicList = []

    #Initialize the dictionaries with the two while loops below
    i = 0
    while i < 6:
      colorDicList.append({key: 0 for key in colorKeys})
      i = i + 1

    i = 0
    while i < 6:
      spTypeDicList.append({key: 0 for key in spTypeKeys})
      i = i + 1
    
    # Get the number of occurences of each class
    i = 0
    while i < len(x_train):
      if y_train.iat[i] == 0:
        quantities[0] = quantities[0] + 1
      elif y_train.iat[i] == 1:
        quantities[1] = quantities[1] + 1
      elif y_train.iat[i] == 2:
        quantities[2] = quantities[2] + 1
      elif y_train.iat[i] == 3:
        quantities[3] = quantities[3] + 1
      elif y_train.iat[i] == 4:
        quantities[4] = quantities[4] + 1
      elif y_train.iat[i] == 5:
        quantities[5] = quantities[5] + 1
      i = i + 1

    # Get the number of occurences of each color and spectral type
    i = 0
    while i < len(x_train):
      starType = y_train.iat[i]
      if x_train.iat[i, 4] == 'Blue White':
        colorDicList[starType]['Blue White'] += 1
      elif x_train.iat[i, 4] == 'White':
        colorDicList[starType]['White'] += 1
      elif x_train.iat[i, 4] == 'Red':
        colorDicList[starType]['Red'] += 1
      elif x_train.iat[i, 4] == 'Blue':
        colorDicList[starType]['Blue'] += 1
      elif x_train.iat[i, 4] == 'yellow-white':
        colorDicList[starType]['yellow-white'] += 1
      elif x_train.iat[i, 4] == 'White-Yellow':
        colorDicList[starType]['White-Yellow'] += 1
      elif x_train.iat[i, 4] == 'Orange':
        colorDicList[starType]['Orange'] += 1
      elif x_train.iat[i, 4] == 'Orange-Red':
        colorDicList[starType]['Orange-Red'] += 1
      elif x_train.iat[i, 4] == 'Yellowish White':
        colorDicList[starType]['Yellowish White'] += 1
      elif x_train.iat[i, 4] == 'Pale yellow orange':
        colorDicList[starType]['Pale yellow orange'] += 1
      elif x_train.iat[i, 4] == 'Yellowish':
        colorDicList[starType]['Yellowish'] += 1
      elif x_train.iat[i, 4] == 'Whitish':
        colorDicList[starType]['Whitish'] += 1
      
      if x_train.iat[i, 5] == 'O':
        spTypeDicList[starType]['O'] += 1
      elif x_train.iat[i, 5] == 'B':
        spTypeDicList[starType]['B'] += 1
      elif x_train.iat[i, 5] == 'A':
        spTypeDicList[starType]['A'] += 1
      elif x_train.iat[i, 5] == 'F':
        spTypeDicList[starType]['F'] += 1
      elif x_train.iat[i, 5] == 'G':
        spTypeDicList[starType]['G'] += 1
      elif x_train.iat[i, 5] == 'K':
        spTypeDicList[starType]['K'] += 1
      elif x_train.iat[i, 5] == 'M':
        spTypeDicList[starType]['M'] += 1
      i = i + 1

    priorProbabilities = [0, 0, 0, 0, 0, 0] # The prior probabilities
    
    # Calculate the prior probabilities
    i = 0
    while i < len(priorProbabilities):
      priorProbabilities[i] = quantities[i] / len(x_train)
      i = i + 1
    
    # Calculate the means of each class in each column
    c = 0
    while c <= 3:
      r = 0
      current_column_means = [0, 0, 0, 0, 0, 0] # means of each class for the current column
      # Get the sum of column values for each class for the current column
      while r < len(x_train):
        if y_train.iat[r] == 0:
          current_column_means[0] += x_train.iat[r, c]
        elif y_train.iat[r] == 1:
          current_column_means[1] += x_train.iat[r, c]
        elif y_train.iat[r] == 2:
          current_column_means[2] += x_train.iat[r, c]
        elif y_train.iat[r] == 3:
          current_column_means[3] += x_train.iat[r, c]
        elif y_train.iat[r] == 4:
          current_column_means[4] += x_train.iat[r, c]
        elif y_train.iat[r] == 5:
          current_column_means[5] += x_train.iat[r, c]
        r = r + 1
      # Divide each sum by their corresponding class's quantity to get the means
      current_column_means[0] /= quantities[0]
      current_column_means[1] /= quantities[1]
      current_column_means[2] /= quantities[2]
      current_column_means[3] /= quantities[3]
      current_column_means[4] /= quantities[4]
      current_column_means[5] /= quantities[5]
      
      means.append(current_column_means)
      c = c + 1  

    # Get the standard deviations for each class in each column
    c = 0
    while c <= 3:
      r = 0
      current_column_stds = [0, 0, 0, 0, 0, 0] # standard deviation of each class in the current column
      # Calculate the sum of (value - mean)^2 for each class
      while r < len(x_train):
        if y_train.iat[r] == 0:
          current_column_stds[0] += (x_train.iat[r, c] - means[c][0])**2
        elif y_train.iat[r] == 1:
          current_column_stds[1] += (x_train.iat[r, c] - means[c][1])**2
        elif y_train.iat[r] == 2:
          current_column_stds[2] += (x_train.iat[r, c] - means[c][2])**2
        elif y_train.iat[r] == 3:
          current_column_stds[3] += (x_train.iat[r, c] - means[c][3])**2
        elif y_train.iat[r] == 4:
          current_column_stds[4] += (x_train.iat[r, c] - means[c][4])**2
        elif y_train.iat[r] == 5:
          current_column_stds[5] += (x_train.iat[r, c] - means[c][5])**2
        r = r + 1

      # Perform the rest of the standard deviation formula
      current_column_stds[0] = math.sqrt(current_column_stds[0] / quantities[0])
      current_column_stds[1] = math.sqrt(current_column_stds[1] / quantities[1])
      current_column_stds[2] = math.sqrt(current_column_stds[2] / quantities[2])
      current_column_stds[3] = math.sqrt(current_column_stds[3] / quantities[3])
      current_column_stds[4] = math.sqrt(current_column_stds[4] / quantities[4])
      current_column_stds[5] = math.sqrt(current_column_stds[5] / quantities[5])
      
      stds.append(current_column_stds)
      c = c + 1
    
    predictedClasses = [] # The classes predicted for each test sample

    # normal probability distribution function  to calculate the probability of
    # the current test sample belonging to each class
    r = 0
    while r < len(x_test):
      c = 0
      class_probabilities = [1, 1, 1, 1, 1, 1] # Initialize class probabilities

      # Calculate the product of normal distribution functions for each class
      # for columns 0-3
      while c <= 3:
        class_probabilities[0] *= norm.pdf(x_test.iat[r, c], means[c][0], stds[c][0])
        class_probabilities[1] *= norm.pdf(x_test.iat[r, c], means[c][1], stds[c][1])
        class_probabilities[2] *= norm.pdf(x_test.iat[r, c], means[c][2], stds[c][2])
        class_probabilities[3] *= norm.pdf(x_test.iat[r, c], means[c][3], stds[c][3])
        class_probabilities[4] *= norm.pdf(x_test.iat[r, c], means[c][4], stds[c][4])
        class_probabilities[5] *= norm.pdf(x_test.iat[r, c], means[c][5], stds[c][5])
        c = c + 1
      
      # Get the probability values for each class from the Color column
      if c == 4:
        color = x_test.iat[r, c]
        if color in colorDicList[0]:
          if color == 'Blue White':
            class_probabilities[0] *= (colorDicList[0]['Blue White'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['Blue White'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['Blue White'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['Blue White'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['Blue White'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['Blue White'] / quantities[5])
          elif color == 'White':
            class_probabilities[0] *= (colorDicList[0]['White'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['White'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['White'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['White'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['White'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['White'] / quantities[5])
          elif color == 'Red':
            class_probabilities[0] *= (colorDicList[0]['Red'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['Red'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['Red'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['Red'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['Red'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['Red'] / quantities[5])
          elif color == 'Blue':
            class_probabilities[0] *= (colorDicList[0]['Blue'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['Blue'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['Blue'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['Blue'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['Blue'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['Blue'] / quantities[5])
          elif color == 'yellow-white':
            class_probabilities[0] *= (colorDicList[0]['yellow-white'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['yellow-white'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['yellow-white'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['yellow-white'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['yellow-white'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['yellow-white'] / quantities[5])
          elif color == 'White-Yellow':
            class_probabilities[0] *= (colorDicList[0]['White-Yellow'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['White-Yellow'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['White-Yellow'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['White-Yellow'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['White-Yellow'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['White-Yellow'] / quantities[5])
          elif color == 'Orange':
            class_probabilities[0] *= (colorDicList[0]['Orange'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['Orange'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['Orange'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['Orange'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['Orange'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['Orange'] / quantities[5])
          elif color == 'Orange-Red':
            class_probabilities[0] *= (colorDicList[0]['Orange-Red'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['Orange-Red'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['Orange-Red'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['Orange-Red'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['Orange-Red'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['Orange-Red'] / quantities[5])
          elif color == 'Yellowish White':
            class_probabilities[0] *= (colorDicList[0]['Yellowish White'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['Yellowish White'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['Yellowish White'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['Yellowish White'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['Yellowish White'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['Yellowish White'] / quantities[5])
          elif color == 'Pale yellow orange':
            class_probabilities[0] *= (colorDicList[0]['Pale yellow orange'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['Pale yellow orange'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['Pale yellow orange'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['Pale yellow orange'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['Pale yellow orange'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['Pale yellow orange'] / quantities[5])
          elif color == 'Yellowish':
            class_probabilities[0] *= (colorDicList[0]['Yellowish'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['Yellowish'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['Yellowish'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['Yellowish'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['Yellowish'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['Yellowish'] / quantities[5])
          elif color == 'Whitish':
            class_probabilities[0] *= (colorDicList[0]['Whitish'] / quantities[0])
            class_probabilities[1] *= (colorDicList[1]['Whitish'] / quantities[1])
            class_probabilities[2] *= (colorDicList[2]['Whitish'] / quantities[2])
            class_probabilities[3] *= (colorDicList[3]['Whitish'] / quantities[3])
            class_probabilities[4] *= (colorDicList[4]['Whitish'] / quantities[4])
            class_probabilities[5] *= (colorDicList[5]['Whitish'] / quantities[5])
        c = c + 1
      
      # Get the probability values for each class from the Spectral Type column
      if c == 5:
        spType = x_test.iat[r, c]
        if spType in spTypeDicList[0]:
          if spType == 'O':
            class_probabilities[0] *= (spTypeDicList[0]['O'] / quantities[0])
            class_probabilities[1] *= (spTypeDicList[1]['O'] / quantities[1])
            class_probabilities[2] *= (spTypeDicList[2]['O'] / quantities[2])
            class_probabilities[3] *= (spTypeDicList[3]['O'] / quantities[3])
            class_probabilities[4] *= (spTypeDicList[4]['O'] / quantities[4])
            class_probabilities[5] *= (spTypeDicList[5]['O'] / quantities[5])
          elif spType == 'B':
            class_probabilities[0] *= (spTypeDicList[0]['B'] / quantities[0])
            class_probabilities[1] *= (spTypeDicList[1]['B'] / quantities[1])
            class_probabilities[2] *= (spTypeDicList[2]['B'] / quantities[2])
            class_probabilities[3] *= (spTypeDicList[3]['B'] / quantities[3])
            class_probabilities[4] *= (spTypeDicList[4]['B'] / quantities[4])
            class_probabilities[5] *= (spTypeDicList[5]['B'] / quantities[5])
          elif spType == 'A':
            class_probabilities[0] *= (spTypeDicList[0]['A'] / quantities[0])
            class_probabilities[1] *= (spTypeDicList[1]['A'] / quantities[1])
            class_probabilities[2] *= (spTypeDicList[2]['A'] / quantities[2])
            class_probabilities[3] *= (spTypeDicList[3]['A'] / quantities[3])
            class_probabilities[4] *= (spTypeDicList[4]['A'] / quantities[4])
            class_probabilities[5] *= (spTypeDicList[5]['A'] / quantities[5])
          elif spType == 'F':
            class_probabilities[0] *= (spTypeDicList[0]['F'] / quantities[0])
            class_probabilities[1] *= (spTypeDicList[1]['F'] / quantities[1])
            class_probabilities[2] *= (spTypeDicList[2]['F'] / quantities[2])
            class_probabilities[3] *= (spTypeDicList[3]['F'] / quantities[3])
            class_probabilities[4] *= (spTypeDicList[4]['F'] / quantities[4])
            class_probabilities[5] *= (spTypeDicList[5]['F'] / quantities[5])
          elif spType == 'G':
            class_probabilities[0] *= (spTypeDicList[0]['G'] / quantities[0])
            class_probabilities[1] *= (spTypeDicList[1]['G'] / quantities[1])
            class_probabilities[2] *= (spTypeDicList[2]['G'] / quantities[2])
            class_probabilities[3] *= (spTypeDicList[3]['G'] / quantities[3])
            class_probabilities[4] *= (spTypeDicList[4]['G'] / quantities[4])
            class_probabilities[5] *= (spTypeDicList[5]['G'] / quantities[5])
          elif spType == 'K':
            class_probabilities[0] *= (spTypeDicList[0]['K'] / quantities[0])
            class_probabilities[1] *= (spTypeDicList[1]['K'] / quantities[1])
            class_probabilities[2] *= (spTypeDicList[2]['K'] / quantities[2])
            class_probabilities[3] *= (spTypeDicList[3]['K'] / quantities[3])
            class_probabilities[4] *= (spTypeDicList[4]['K'] / quantities[4])
            class_probabilities[5] *= (spTypeDicList[5]['K'] / quantities[5])
          elif spType == 'M':
            class_probabilities[0] *= (spTypeDicList[0]['M'] / quantities[0])
            class_probabilities[1] *= (spTypeDicList[1]['M'] / quantities[1])
            class_probabilities[2] *= (spTypeDicList[2]['M'] / quantities[2])
            class_probabilities[3] *= (spTypeDicList[3]['M'] / quantities[3])
            class_probabilities[4] *= (spTypeDicList[4]['M'] / quantities[4])
            class_probabilities[5] *= (spTypeDicList[5]['M'] / quantities[5])
        c = c + 1

      # Multiply by the prior probabilities
      class_probabilities[0] *= priorProbabilities[0]
      class_probabilities[1] *= priorProbabilities[1]
      class_probabilities[2] *= priorProbabilities[2]
      class_probabilities[3] *= priorProbabilities[3]
      class_probabilities[4] *= priorProbabilities[4]
      class_probabilities[5] *= priorProbabilities[5] 

      # Find the maximum probability value
      maxProb = max(class_probabilities)
      maxProbIndex = class_probabilities.index(maxProb)
      
      # Append the highest probability value's class to the predicted classes
      if maxProbIndex == 0:
        predictedClasses.append(0)
      elif maxProbIndex == 1:
        predictedClasses.append(1)
      elif maxProbIndex == 2:
        predictedClasses.append(2)
      elif maxProbIndex == 3:
        predictedClasses.append(3)
      elif maxProbIndex == 4:
        predictedClasses.append(4)
      elif maxProbIndex == 5:
        predictedClasses.append(5)
      r = r + 1
    
    # Calculate the accuracy
    r = 0
    correct = 0
    while r < len(x_test):
      if y_test.iat[r] == predictedClasses[r]:
        correct = correct + 1
      r = r + 1

    accuracy = correct / len(x_test)
    createConfusionMatrix(y_test, predictedClasses) # Get confusion matrix
    print()
    print(f"Naive Bayes accuracy: {accuracy * 100}") # Print final accuracy
    print()

"""## K-Nearest Neighbors"""

def kNN(x_train, x_test, y_train, y_test):

    k = 7 # k value is set

    # index positions from before the split are maintained
    # reset_index resets all the index values to restore proper ordering
    x_train = x_train.reset_index(drop=True)
    x_test = x_test.reset_index(drop=True)
    y_train = y_train.reset_index(drop=True)
    y_test = y_test.reset_index(drop=True)
    
    # Merge the train and test lists
    x_train['Type'] = y_train
    x_test['Type'] = y_test
    
    # Convert Spectral Type characters to integers
    knn_training = spTypeNum(x_train)
    knn_testing = spTypeNum(x_test) 

    # Get the final training and testing data without Color 
    knn_training = knn_training.drop(columns=['Color'])
    knn_testing = knn_testing.drop(columns=['Color'])

    predictedClasses = [] # The predicted classes

    # Determining the predicted classes
    i = 0
    while i < len(knn_testing):
      # Initialize distances list and get the current sample from test data
      j = 0
      test_point = np.array(knn_testing.iloc[i])
      test_point = np.delete(test_point, 5)
      distances = [] # Distance and class from each training sample stored here
      # Calculate the distances between the test sample and each training sample
      while j < len(knn_training):
        training_point = np.array(knn_training.iloc[j]) # Get the training sample
        classLabel = training_point[5] # Get the training sample's class
        training_point = np.delete(training_point, 5) # Drop the class label
        distance = np.linalg.norm(training_point - test_point) # Calculate distance 
        distances.append([distance, classLabel]) # Append distance and class label to distances
        j = j + 1
      sorted_distances = sorted(distances,key=lambda x: (x[0],x[1])) # Sort the distances in ascending order
      
      classes = [] # Classes for top k values in distances

      # The class that appears most often in classes is the predicted class
      l = 0
      while l < k:
        classes.append(sorted_distances[l][1])
        l = l + 1
      majority_class = mode(classes)
      predictedClasses.append(majority_class)
      i = i + 1

    # Calculate the accuracy
    r = 0
    correct = 0
    while r < len(knn_testing):
      if knn_testing.iat[r, 5] == predictedClasses[r]:
        correct = correct + 1
      r = r + 1

    accuracy = correct / len(knn_testing)
    createConfusionMatrix(y_test, predictedClasses) # Get confusion matrix
    print()
    print(f"KNN accuracy: {accuracy * 100}") # Print final accuracy
    print()

"""## Support Vector Machine"""

def SVM(x_train, x_test, y_train, y_test):
  # index positions from before the split are maintained
  # reset_index resets all the index values to restore proper ordering
  x_train = x_train.reset_index(drop=True)
  x_test = x_test.reset_index(drop=True)
  y_train = y_train.reset_index(drop=True)
  y_test = y_test.reset_index(drop=True)

  # Convert Spectral Type characters to integers
  x_train = spTypeNum(x_train)
  x_test = spTypeNum(x_test)
  
  # Drop the Color column
  x_train = x_train.drop(columns=['Color'])
  x_test = x_test.drop(columns=['Color'])

  # Fit the training and testing data in SVC classifier
  SVC_clf = SVC(kernel='linear') 
  SVC_clf.fit(x_train, y_train)

  # Predict the classes for testing samples
  SVC_prediction = SVC_clf.predict(x_test)
  createConfusionMatrix(y_test, SVC_prediction) # Get confusion matrix
  print()
  print(f"Support Vector Machine accuracy : {accuracy_score(y_test, SVC_prediction) * 100}") # Print final accuracy
  print()

"""## Decision Tree

"""

def DecTree(x_train, x_test, y_train, y_test):
  # index positions from before the split are maintained
  # reset_index resets all the index values to restore proper ordering
  x_train = x_train.reset_index(drop=True)
  x_test = x_test.reset_index(drop=True)
  y_train = y_train.reset_index(drop=True)
  y_test = y_test.reset_index(drop=True)

  # Convert Spectral Type characters to integers
  x_train = spTypeNum(x_train)
  x_test = spTypeNum(x_test)

  # Drop the Color column
  x_train = x_train.drop(columns=['Color'])
  x_test = x_test.drop(columns=['Color'])

  # Fit the training and testing data in Decision Tree classifier
  DecTreeClf = tree.DecisionTreeClassifier()
  DecTreeClf.fit(x_train, y_train)

  # Predict the classes for testing samples
  DecTreePrediction = DecTreeClf.predict(x_test)
  createConfusionMatrix(y_test, DecTreePrediction) # Get confusion matrix
  print()
  print(f"Decision Tree accuracy : {accuracy_score(y_test, DecTreePrediction) * 100}") # Print final accuracy
  print()

"""## Main Code

"""

import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold
import numpy as np
from sklearn.metrics import accuracy_score, classification_report
from statistics import mode
from sklearn.svm import SVC  
from sklearn import tree
import math
from scipy.stats import norm
import numpy as np

df_stars = pd.read_csv("Stars.csv") # Save Stars dataset as dataframe

# Create copy of Stars dataframe
# Color, Spectral Class, and Type are dropped in the copy
df_stars_copy = df_stars.drop(columns=['Color', 'Spectral_Class', 'Type'])

#min-max normalization on dataframe copy
for column in df_stars_copy.columns:
  df_stars_copy[column] = (df_stars_copy[column] - df_stars_copy[column].min()) / (df_stars_copy[column].max() - df_stars_copy[column].min())

# Overwrite original dataframe with normalized data
df_stars.loc[:, ['Temperature']] = df_stars_copy[['Temperature']] 
df_stars.loc[:, ['L']] = df_stars_copy[['L']] 
df_stars.loc[:, ['R']] = df_stars_copy[['R']] 
df_stars.loc[:, ['A_M']] = df_stars_copy[['A_M']]

# Some of the colors appear with slightly different formats
# This loop creates a single label for each repeating color
i = 0
while i < len(df_stars):
  if df_stars.at[i, 'Color'] == 'Blue-white' or df_stars.at[i, 'Color'] == 'Blue-White' or df_stars.at[i, 'Color'] == 'Blue white':
    df_stars.at[i, 'Color'] = 'Blue White'
  elif df_stars.at[i, 'Color'] == 'white':
    df_stars.at[i, 'Color'] = 'White'
  elif df_stars.at[i, 'Color'] == 'yellowish':
    df_stars.at[i, 'Color'] = 'Yellowish'
  i = i + 1

# x = Star dataframe without class label
# y = class label
x = df_stars.drop(columns=['Type'])
y = df_stars['Type']

# stratified K Fold for 10 splits
skf = StratifiedKFold(n_splits=10)

fold = 1 # The current fold

# Split the data 10 times for 10 folds
# Call each classifier method for each fold
# Confusion Matrices and accuracies of each classifier are outputted for each fold
for train_index, test_index in skf.split(x, y):
    x_train, x_test = x.iloc[train_index], x.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    print('Fold: ', fold)
    print()
    naiveBayes(x_train, x_test, y_train, y_test)
    kNN(x_train, x_test, y_train, y_test)
    SVM(x_train, x_test, y_train, y_test)
    DecTree(x_train, x_test, y_train, y_test)
    print('\n---------------------\n')
    fold = fold + 1